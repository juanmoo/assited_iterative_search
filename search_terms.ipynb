{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8d67fa",
   "metadata": {},
   "source": [
    "# Takeda Part 2: Iterative Keyword Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebca42",
   "metadata": {},
   "source": [
    "## Part I: Search (keyword -> papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8e377a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import yake\n",
    "import pandas as pd\n",
    "\n",
    "_email = ''\n",
    "_db = 'pubmed'\n",
    "_retmode = 'xml'\n",
    "_sort = 'relevance'\n",
    "_retmax = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d391689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Keywords\n",
    "config = {\n",
    "    'lan': 'en',\n",
    "    'n': 3,\n",
    "    'dedupLim': 0.9,\n",
    "    'dedupFunc': 'seqm',\n",
    "    'windowsSize': 1,\n",
    "    'top': 100,\n",
    "    'features': None\n",
    "}\n",
    "\n",
    "config_100 = {**config}\n",
    "kwe_100 = yake.KeywordExtractor(**config_100)\n",
    "\n",
    "config_20 = {**config}\n",
    "config_20['top'] = 20\n",
    "kwe_20 = yake.KeywordExtractor(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "420013c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    Entrez.email = _email\n",
    "    handle = Entrez.esearch(db=_db,\n",
    "                           sort=_sort,\n",
    "                           retmax=_retmax,\n",
    "                           retmode=_retmode,\n",
    "                           term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = _email\n",
    "    handle = Entrez.efetch(db=_db,\n",
    "                          retmode=_retmode,\n",
    "                          id=ids)\n",
    "    papers_details = Entrez.read(handle)\n",
    "    papers = []\n",
    "    \n",
    "    for j, paper in enumerate(papers_details['PubmedArticle']):\n",
    "        article = paper['MedlineCitation']['Article']\n",
    "        abstract_text = ' '.join(article['Abstract']['AbstractText'])\n",
    "        \n",
    "        # Listed Keywords\n",
    "        kw_lists = paper['MedlineCitation']['KeywordList']\n",
    "        kws = sorted(set([str(kw) for kw_list in kw_lists for kw in kw_list]))\n",
    "        \n",
    "        # Get Keywords from Title and Abstract\n",
    "        all_text = str(article['ArticleTitle']) + ' ' + abstract_text\n",
    "        all_text = re.sub('\\s+', ' ', all_text)\n",
    "        all_text = ' '.join(word_tokenize(all_text)).lower()\n",
    "        keywords = list(map(lambda e: e[0], kw_extractor.extract_keywords(all_text)))\n",
    "        kws.extend(keywords)\n",
    "        \n",
    "        papers.append({\n",
    "            'id': id_list[j],\n",
    "            'title': article['ArticleTitle'],\n",
    "            'abstract': abstract_text,\n",
    "            'keywords': kws[:15]\n",
    "        })\n",
    "        \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fe2e81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = '(PFS[Title/Abstract]) AND (CLINICAL TRIAL[Title/Abstract])'\n",
    "res = search(query)\n",
    "id_list = res['IdList']\n",
    "papers = fetch_details(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "08b661fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883b7c3",
   "metadata": {},
   "source": [
    "## Part II: Keyword Extraction (papers -> keywords)\n",
    "\n",
    "#### Current Approach\n",
    "PubMed entries come with keywords per along with each paper entry. For now, only those keywords will be utilized during the search.\n",
    "\n",
    "#### TODO:\n",
    "* Expand keyword instances by utilizing RAKE/YAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea979154",
   "metadata": {},
   "source": [
    "## Part III: Filter Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6a43fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge ipywidgets -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18fb9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy\n",
      "  Downloading sympy-1.9-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[K     |████████████████████████████████| 532 kB 60.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy\n",
      "Successfully installed mpmath-1.2.1 sympy-1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e28e4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "import sympy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51f17e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, id, title, abstract, keywords):\n",
    "        self.id = id\n",
    "        self.abstract = abstract\n",
    "        self.title = title\n",
    "        self.keywords = keywords\n",
    "    \n",
    "    def get_document_text(self):\n",
    "        return re.sub('\\s+', ' ', self.title + ' ' + self.abstract).strip()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'<Document id:{self.id}\\ttitle: {self.title[:50]}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "fc1b6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.document_decisions = []\n",
    "        \n",
    "    def add_document(self, document, decision):\n",
    "                \n",
    "        # Save document\n",
    "        self.document_decisions.append(decision)\n",
    "        self.documents.append(document)\n",
    "        \n",
    "        \n",
    "    def create_query_v2(self):\n",
    "        \n",
    "        texts = [d.get_document_text() for d in self.documents]\n",
    "        all_text = ' '.join(texts)\n",
    "        \n",
    "        keywords = list(map(lambda e: e[0], kw_extractor.extract_keywords(all_text)))\n",
    "        kw_to_id = {}\n",
    "        \n",
    "        \n",
    "        print(keywords)\n",
    "        \n",
    "    def create_query(self):\n",
    "        return self.create_query_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d94ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QueryBuilder()\n",
    "docs = [Document(**kwargs) for kwargs in papers]\n",
    "doc_decisions = [random.random() > 0.5 for _ in range(len(docs))]\n",
    "\n",
    "for doc, dec in zip(docs, doc_decisions):\n",
    "    qb.add_document(doc, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1989f",
   "metadata": {},
   "source": [
    "### Proof of Concept Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c9b3ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(papers)\n",
    "texts = df['title'] + ' ' + df['abstract']\n",
    "all_text = ' '.join(texts)\n",
    "keywords_e = kwe_100.extract_keywords(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "35ca43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create karnaugh Map of keywords in documents\n",
    "keyword_to_id = {k[0]:j for j, k in enumerate(keywords_e)}\n",
    "kwo = np.zeros((len(texts), len(keywords_e)))\n",
    "for t_idx, text in enumerate(texts):\n",
    "    for k, k_idx in keyword_to_id.items():\n",
    "        if k in text:\n",
    "            kwo[t_idx, k_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "34452666",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = np.array([k[0] for k in keywords_e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ef0b79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first run, take the OR of the most prominent keywords\n",
    "dnf = [(k,) for k in keywords[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "72446b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query(dnf):\n",
    "    \n",
    "    dnf_clauses = []\n",
    "    for clause in dnf:\n",
    "        literals = []\n",
    "        for lit in clause:\n",
    "            lit_str = f'({lit}[Title/Abstract])'\n",
    "            literals.append(lit_str)\n",
    "        clause_str = ' AND '.join(literals)\n",
    "        dnf_clauses.append(clause_str)\n",
    "    dnf_str = ' OR '.join([f'({c})' for c in dnf_clauses])\n",
    "    \n",
    "    return dnf_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = construct_query(dnf)\n",
    "res = search(query)\n",
    "id_list = res['IdList']\n",
    "papers_new = fetch_details(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933372fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e3785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11",
   "language": "python",
   "name": "cuda11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
